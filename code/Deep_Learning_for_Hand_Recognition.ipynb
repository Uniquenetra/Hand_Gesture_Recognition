{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACry_vIswInt"
      },
      "source": [
        "# Deep Learning for Hand Gesture Recognition with PyTorch: Quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIqcb9y4wInu"
      },
      "source": [
        "This notebook is inherited from a demo pytorch implementation of the deep learning model for hand gesture recognition introduced in the article [Deep Learning for Hand Gesture Recognition on Skeletal Data](https://ieeexplore.ieee.org/document/8373818) from G. Devineau, F. Moutarde, W. Xi and J. Yang.\n",
        "\n",
        "If you find this code useful in your research, please consider citing:\n",
        "\n",
        "```\n",
        "@inproceedings{devineau2018deep,\n",
        "  title={Deep learning for hand gesture recognition on skeletal data},\n",
        "  author={Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},\n",
        "  booktitle={2018 13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n",
        "  pages={106--113},\n",
        "  year={2018},\n",
        "  organization={IEEE}\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLpq_g4hDA8n"
      },
      "source": [
        "### 0. Understanding the model\n",
        "\n",
        "You can find an intuitive explanation of the model at: https://github.com/guillaumephd/deep_learning_hand_gesture_recognition\n",
        "\n",
        "* Training Dataset Specification from: http://www-rech.telecom-lille.fr/shrec2017-hand/\n",
        "\n",
        "\n",
        "The dataset contains sequences of 14 hand gestures performed in two ways: using one finger and the whole hand. Each gesture is performed between 1 and 10 times by 28 participants in 2 ways - described above - , resulting in 2800 sequences. All participants are right handed. Sequences are labelled following their gesture, the number of fingers used, the performer and the trial. Each frame of sequences contains a depth image, the coordinates of 22 joints both in the 2D depth image space and in the 3D world space forming a full hand skeleton. The Intel RealSense short range depth camera is used to collect our dataset. The depth images and hand skeletons were captured at 30 frames per second, with a resolution of the depth image of 640x480. The length of sample gestures ranges from 20 to 50 frames.\n",
        "\n",
        "\n",
        "For a sequence of size N:\n",
        "\n",
        "* depth_n.png contains the depth image of the nth frame of the sequence.\n",
        "* general_informations.txt contains a matrix of size Nx5 (one line by frame). The format is as follows: Timestamp in 10-7 seconds and hand region of interest in the depth image (x, y, width, height).\n",
        "* skeletons_image.txt contains a matrix of size Nx44. Each line contains the 2D hand joints coordinates in the depth image space. The format is as follows: x1 y1 z1 - x2 y2 z2 - ... - x22 y22 z22.\n",
        "* skeletons_world.txt contains a matrix of size Nx66. Each line contains the 3D hand joints coordinates in the world space. The format is as follows: x1 y1 z1 - x2 y2 z2 - ... - x22 y22 z22.\n",
        "\n",
        "  The order of the joints in the line is: \n",
        "1.Wrist,\n",
        "\n",
        "2.Palm, \n",
        "\n",
        "3.thumb_base, \n",
        "\n",
        "4.thumb_first_joint, \n",
        "\n",
        "5.thumb_second_joint, \n",
        "\n",
        "6.thumb_tip, \n",
        "\n",
        "7.index_base, \n",
        "\n",
        "8.index_first_joint, \n",
        "\n",
        "9.index_second_joint, \n",
        "\n",
        "10.index_tip, \n",
        "\n",
        "11.middle_base, \n",
        "\n",
        "12.middle_first_joint, \n",
        "\n",
        "13.middle_second_joint, \n",
        "\n",
        "14.middle_tip, \n",
        "\n",
        "15.ring_base, \n",
        "\n",
        "16.ring_first_joint, \n",
        "\n",
        "17.ring_second_joint, \n",
        "\n",
        "18.ring_tip, \n",
        "\n",
        "19.pinky_base, \n",
        "\n",
        "20.pinky_first_joint, \n",
        "\n",
        "21.pinky_second_joint, \n",
        "\n",
        "22.pinky_tip.\n",
        "\n",
        "* train_gestures.txt and test_gestures.txt contains information about the train and the test sequences. These files contains respectively 1960 (70%) and 840 (30%) lines. Each line follow the following pattern: id_gesture     id_finger    id_subject    id_essai    14_labels    28_labels    size_sequence\n",
        "* display_sequence.m and display_sequence.py is respectively a matlab and a python script which charge and display a sequence. Dependencies for the python script: Scipy, Numpy and Matplotlib.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIPpe_3nwIny"
      },
      "source": [
        "### 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RLdxMOfwInz"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import sys\n",
        "if sys.version_info.major < 3:\n",
        "    print('You are using python 2, but you should rather use python 3.')\n",
        "    print('    If you still want to use python 2, ensure you import:')\n",
        "    print('    >> from __future__ import unicode_literals, print_function, division')\n",
        "\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import itertools\n",
        "import time\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from hashlib import new\n",
        "import matplotlib.pyplot as plt \n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fmhkYESwIn3"
      },
      "source": [
        "If you encounter an python error regarding a missing module at some point, uncomment the appropriate line in the cell below and run it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkSFp85JwIn9"
      },
      "outputs": [],
      "source": [
        "# (bonus) plot acc with tensorboard\n",
        "#   Command to start tensorboard if installed (requires tensorflow):\n",
        "#   $  tensorboard --logdir ./runs\n",
        "try:\n",
        "    from tensorboardX import SummaryWriter\n",
        "except:\n",
        "    # tensorboardX is not installed, just fail silently\n",
        "    class SummaryWriter():\n",
        "        def __init__(self):\n",
        "            pass\n",
        "        def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pm0OpBtKzbB"
      },
      "source": [
        "### 2. Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCh7Bc28PoNP"
      },
      "source": [
        "If you use a custom dataset, you'll likely want to change `n_channels` and `n_classes` to match your values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDZVj3eNKylZ"
      },
      "outputs": [],
      "source": [
        "n_classes = 14\n",
        "duration = 100\n",
        "n_channels = 66\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WJg_BK8PUhd"
      },
      "source": [
        "### 3. Create a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTqC9q7HPVno"
      },
      "outputs": [],
      "source": [
        "class HandGestureNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    [Devineau et al., 2018] Deep Learning for Hand Gesture Recognition on Skeletal Data\n",
        "\n",
        "    Summary\n",
        "    -------\n",
        "        Deep Learning Model for Hand Gesture classification using pose data only (no need for RGBD)\n",
        "        The model computes a succession of [convolutions and pooling] over time independently on each of the 66 (= 22 * 3) sequence channels.\n",
        "        Each of these computations are actually done at two different resolutions, that are later merged by concatenation\n",
        "        with the (pooled) original sequence channel.\n",
        "        Finally, a multi-layer perceptron merges all of the processed channels and outputs a classification.\n",
        "    \n",
        "    TL;DR:\n",
        "    ------\n",
        "        input ------------------------------------------------> split into n_channels channels [channel_i]\n",
        "            channel_i ----------------------------------------> 3x [conv/pool/dropout] low_resolution_i\n",
        "            channel_i ----------------------------------------> 3x [conv/pool/dropout] high_resolution_i\n",
        "            channel_i ----------------------------------------> pooled_i\n",
        "            low_resolution_i, high_resolution_i, pooled_i ----> output_channel_i\n",
        "        MLP(n_channels x [output_channel_i]) -------------------------> classification\n",
        "\n",
        "    Article / PDF:\n",
        "    --------------\n",
        "        https://ieeexplore.ieee.org/document/8373818\n",
        "\n",
        "    Please cite:\n",
        "    ------------\n",
        "        @inproceedings{devineau2018deep,\n",
        "            title={Deep learning for hand gesture recognition on skeletal data},\n",
        "            author={Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},\n",
        "            booktitle={2018 13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n",
        "            pages={106--113},\n",
        "            year={2018},\n",
        "            organization={IEEE}\n",
        "        }\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_channels=66, n_classes=14, dropout_probability=0.2):\n",
        "\n",
        "        super(HandGestureNet, self).__init__()\n",
        "        \n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_probability = dropout_probability\n",
        "\n",
        "        # Layers ----------------------------------------------\n",
        "        self.all_conv_high = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=7, padding=3),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=7, padding=3),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=self.dropout_probability),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.all_conv_low = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=self.dropout_probability),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.all_residual = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.AvgPool1d(2),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=9 * n_channels * 12, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=1936, out_features=n_classes)\n",
        "        )\n",
        "\n",
        "        # Initialization --------------------------------------\n",
        "        # Xavier init\n",
        "        for module in itertools.chain(self.all_conv_high, self.all_conv_low, self.all_residual):\n",
        "            for layer in module:\n",
        "                if layer.__class__.__name__ == \"Conv1d\":\n",
        "                    torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "                    torch.nn.init.constant_(layer.bias, 0.1)\n",
        "\n",
        "        for layer in self.fc:\n",
        "            if layer.__class__.__name__ == \"Linear\":\n",
        "                torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "                torch.nn.init.constant_(layer.bias, 0.1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        This function performs the actual computations of the network for a forward pass.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "            input: a tensor of gestures of shape (batch_size, duration, n_channels)\n",
        "                   (where n_channels = 3 * n_joints for 3D pose data)\n",
        "        \"\"\"\n",
        "\n",
        "        # Work on each channel separately\n",
        "        all_features = []\n",
        "\n",
        "        for channel in range(0, self.n_channels):\n",
        "            input_channel = input[:, :, channel]\n",
        "\n",
        "            # Add a dummy (spatial) dimension for the time convolutions\n",
        "            # Conv1D format : (batch_size, n_feature_maps, duration)\n",
        "            input_channel = input_channel.unsqueeze(1)\n",
        "\n",
        "            high = self.all_conv_high[channel](input_channel)\n",
        "            low = self.all_conv_low[channel](input_channel)\n",
        "            ap_residual = self.all_residual[channel](input_channel)\n",
        "\n",
        "            # Time convolutions are concatenated along the feature maps axis\n",
        "            output_channel = torch.cat([\n",
        "                high,\n",
        "                low,\n",
        "                ap_residual\n",
        "            ], dim=1)\n",
        "            all_features.append(output_channel)\n",
        "\n",
        "        # Concatenate along the feature maps axis\n",
        "        all_features = torch.cat(all_features, dim=1)\n",
        "        \n",
        "        # Flatten for the Linear layers\n",
        "        all_features = all_features.view(-1, 9 * self.n_channels * 12)  # <-- 12: depends of the initial sequence length (100).\n",
        "        # If you have shorter/longer sequences, you probably do NOT even need to modify the modify the network architecture:\n",
        "        # resampling your input gesture from T timesteps to 100 timesteps will (surprisingly) probably actually work as well!\n",
        "\n",
        "        # Fully-Connected Layers\n",
        "        output = self.fc(all_features)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0V59z_HPezY"
      },
      "outputs": [],
      "source": [
        "# -------------\n",
        "# Network instantiation\n",
        "# -------------\n",
        "model = HandGestureNet(n_channels=n_channels, n_classes=n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use files from Google Drive"
      ],
      "metadata": {
        "id": "6wTW4B68-MUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijCNbshOReN7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/dhg_data.pckl ./"
      ],
      "metadata": {
        "id": "4RwJI23Oe-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-MZiYI7ALWq"
      },
      "outputs": [],
      "source": [
        "# !cp drive/MyDrive/new_dataset_20_20.pkl ./\n",
        "# !cp drive/MyDrive/new_dataset_100_20.pkl ./\n",
        "!cp drive/MyDrive/new_dataset_100_30.pkl ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/gesture_pretrained_model.pt ./\n",
        "# !cp drive/MyDrive/gesture_trained_20_20.pt ./\n",
        "# !cp drive/MyDrive/gesture_trained_100_20.pt ./\n",
        "# !cp drive/MyDrive/gesture_trained_100_30.pt ./"
      ],
      "metadata": {
        "id": "LMCyw3Snd_m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sABHssjwIoF"
      },
      "source": [
        "### 2. Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WxKirNeGqvl"
      },
      "source": [
        "We load a dataset already created in numpy format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHMs7mOfDOXo"
      },
      "outputs": [],
      "source": [
        "# We load a gesture dataset:\n",
        "#\n",
        "#   x.shape should be (dataset_size, duration, channel)\n",
        "#   y.shape should be (dataset_size, 1)\n",
        "\n",
        "\n",
        "# If you want to use the DHG dataset, go to: https://colab.research.google.com/drive/1ggYG1XRpJ50gVgJqT_uoI257bspNogHj\n",
        "use_dhg_dataset = True\n",
        "# Uncomment the line below to load a new dataset\n",
        "# use_dhg_dataset = False\n",
        "\n",
        "if use_dhg_dataset:\n",
        "    # ------------------------\n",
        "    # DHG Dataset\n",
        "    # ------------------------\n",
        "    try:\n",
        "        # Connect Google Colab instance to Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/gdrive')\n",
        "        # Load the dataset (you already have created in the other notebook) from Google Drive\n",
        "        !cp /MyDrive/dhg_data.pckl\n",
        "        #https://drive.google.com/file/d/1BUxbFwcPRrXn3-ZbwVIOTRgn8zkyBU0_/view?usp=sharing\n",
        "    except:\n",
        "        print(\"You're not in a Google Colab!\")\n",
        "\n",
        "    def load_data(filepath='./shrec_data.pckl'):\n",
        "        \"\"\"\n",
        "        Returns hand gesture sequences (X) and their associated labels (Y).\n",
        "        Each sequence has two different labels.\n",
        "        The first label  Y describes the gesture class out of 14 possible gestures (e.g. swiping your hand to the right).\n",
        "        The second label Y describes the gesture class out of 28 possible gestures (e.g. swiping your hand to the right with your index pointed, or not pointed).\n",
        "        \"\"\"\n",
        "        file = open(filepath, 'rb')\n",
        "        data = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load\n",
        "        file.close()\n",
        "\n",
        "        return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']\n",
        "\n",
        "    #x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = load_data('dhg_data.pckl')\n",
        "    x_train, x_test, y_train_14, y_test_14, y_train_28, y_test_28 = load_data('dhg_data.pckl')\n",
        "    y_train_14, y_test_14 = numpy.array(y_train_14), numpy.array(y_test_14)\n",
        "    y_train_28, y_test_28 = numpy.array(y_train_28), numpy.array(y_test_28)\n",
        "    if n_classes == 14:\n",
        "        y_train = y_train_14\n",
        "        y_test = y_test_14\n",
        "    elif n_classes == 28:\n",
        "        y_train = y_train_28\n",
        "        y_test = y_test_28\n",
        "\n",
        "else:\n",
        "    # ------------------------\n",
        "    # Custom Dataset\n",
        "    # ------------------------\n",
        "    # On the left bar of this colaboratory notebook there is a section called \"Files\".\n",
        "    # Upload your files there and use a path like \"/content/each_file_you_just_uploaded\" to load your data\n",
        "    # \n",
        "    # For now, for the sake of demonstration purposes, let's create fake data\n",
        "\n",
        "    print(\"You're in else!\")\n",
        "\n",
        "    def load_data(filepath='./new_dataset_100_30.pkl'):\n",
        " \n",
        "        file = open(filepath, 'rb')\n",
        "        data = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load\n",
        "        file.close()\n",
        "\n",
        "        return data['x_train'], data['y_train'], data['x_test'], data['y_test']\n",
        "    \n",
        "    x_train, y_train, x_test, y_test = load_data('new_dataset_100_30.pkl')\n",
        "    x_train, x_test = numpy.array(x_train), numpy.array(x_test)\n",
        "    y_train, y_test = numpy.array(y_train), numpy.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prints for dhg_dataset"
      ],
      "metadata": {
        "id": "YPAL-5jw-EIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ORy8Idsv5gs"
      },
      "outputs": [],
      "source": [
        "x_train.shape, x_test.shape, y_train_14.shape, y_test_14.shape, y_train_28.shape, y_test_28.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txHBYr0IGyoe"
      },
      "source": [
        "We now convert numpy data into the torch format, and create a pytorch dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwf7S-ckGnLh"
      },
      "outputs": [],
      "source": [
        "class GestureDataset(Dataset):\n",
        " \n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        " \n",
        "    def __getitem__(self, i):\n",
        "        return self.x[i], self.y[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2mjPB_GGlCM"
      },
      "outputs": [],
      "source": [
        "# ------------------------\n",
        "# Create pytorch datasets and dataloaders:\n",
        "# ------------------------\n",
        "# Convert from numpy to torch format\n",
        "x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
        "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
        "\n",
        "# Ensure the label values are between 0 and n_classes-1\n",
        "if y_train.min() > 0:\n",
        "  y_train = y_train - 1\n",
        "if y_test.min() > 0:\n",
        "  y_test = y_test - 1\n",
        "\n",
        "# Ensure the data type is correct\n",
        "x_train, x_test = x_train.float(), x_test.float()\n",
        "y_train, y_test = y_train.long(), y_test.long()\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = GestureDataset(x=x_train, y=y_train)\n",
        "test_dataset = GestureDataset(x=x_test, y=y_test)\n",
        "\n",
        "# Pytorch dataloaders are used to group dataset items into batches\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_dataloader  = DataLoader(test_dataset,  batch_size=32, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rzzQuy1E8EG"
      },
      "source": [
        "We define some functions we'll need later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dEm1VAcFyrF"
      },
      "outputs": [],
      "source": [
        "def time_since(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '{:02d}m {:02d}s'.format(int(m), int(s))\n",
        "\n",
        "\n",
        "def get_accuracy(model, x, y_ref):\n",
        "    \"\"\"Get the accuracy of the pytorch model on a batch\"\"\"\n",
        "    acc = 0.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predicted = model(x)\n",
        "        _, predicted = predicted.max(dim=1)\n",
        "        acc = 1.0 * (predicted == y_ref).sum().item() / y_ref.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiDdp4xrwIoi"
      },
      "source": [
        "### 5. Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrzXfnl9wIor"
      },
      "source": [
        "Note: reduce the learning rate (in the hyperparameters section) to get smoother accuracy curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkTjgTauwIos"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------\n",
        "# Loss function & Optimizer\n",
        "# -----------------------------------------------------\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l33TTvqq2CTC"
      },
      "source": [
        "Note: this very basic training loop is intended for demonstration purposes only.\n",
        "\n",
        "You should improve it with -at least- an evaluation step which is missing here, and whatever you need/want: cross-validation, early-stopping, model checkpoints, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLWgtKSQwIoy"
      },
      "outputs": [],
      "source": [
        "# -------------\n",
        "# Training\n",
        "# -------------\n",
        "\n",
        "\n",
        "def train(model, criterion, optimizer, dataloader,\n",
        "          x_train, y_train, x_test, y_test,\n",
        "          force_cpu=False, num_epochs=5):\n",
        "    \n",
        "    # use a GPU (for speed) if you have one\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() and not force_cpu else torch.device(\"cpu\")\n",
        "    model = model.to(device)\n",
        "    x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "    x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "\n",
        "    # (bonus) log accuracy values to visualize them in tensorboard:\n",
        "    writer = SummaryWriter()\n",
        "    \n",
        "    # Training starting time\n",
        "    start = time.time()\n",
        "\n",
        "    print('[INFO] Started to train the model.')\n",
        "    print('Training the model on {}.'.format('GPU' if device == torch.device('cuda') else 'CPU'))\n",
        "\n",
        "    data = {\n",
        "            \"Epoch\": [],\n",
        "            \"Time Elapsed\": [],\n",
        "            \"Loss\": [],\n",
        "            \"Accuracy Train\": [],\n",
        "            \"Accuracy Test\": []\n",
        "        }\n",
        "    \n",
        "    for ep in range(num_epochs):\n",
        "\n",
        "        # Ensure we're still in training mode\n",
        "        model.train()\n",
        "\n",
        "        current_loss = 0.0\n",
        "\n",
        "        for idx_batch, batch in enumerate(dataloader):\n",
        "\n",
        "            # Move data to GPU, if available\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # zero the gradient parameters\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            y_pred = model(x)\n",
        "\n",
        "            # backward + optimize\n",
        "            # backward\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            # optimize\n",
        "            optimizer.step()\n",
        "            # for an easy access\n",
        "            current_loss += loss.item()\n",
        "        \n",
        "        train_acc = get_accuracy(model, x_train, y_train)\n",
        "        test_acc = get_accuracy(model, x_test, y_test)\n",
        "        \n",
        "        writer.add_scalar('data/accuracy_train', train_acc, ep)\n",
        "        writer.add_scalar('data/accuracy_test', test_acc, ep)\n",
        "        print('Epoch #{:03d} | Time elapsed : {} | Loss : {:.4e} | Accuracy_train : {:.2f}% | Accuracy_test : {:.2f}% '.format(\n",
        "                ep + 1, time_since(start), current_loss, 100 * train_acc, 100 * test_acc))\n",
        "        data[\"Epoch\"].append(ep + 1)\n",
        "        data[\"Time Elapsed\"].append(time_since(start))\n",
        "        data[\"Loss\"].append(current_loss)\n",
        "        data[\"Accuracy Train\"].append(100 * train_acc)\n",
        "        data[\"Accuracy Test\"].append(100 * test_acc)\n",
        "\n",
        "\n",
        "    print('[INFO] Finished training the model. Total time : {}.'.format(time_since(start)))\n",
        "    # Change the name of the .csv according to the dataset\n",
        "    pd.DataFrame(data).to_csv('prediction_balanced_medium_dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YspFJiWZwIo2"
      },
      "source": [
        "You can now train the model on your dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDAO7GzA3dKq"
      },
      "source": [
        "Note: You can use the GPUs provided by Google Colab to make the training faster: go to the “runtime” dropdown menu, select “change runtime type” and select GPU in the hardware accelerator drop-down menu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb4PTmmjwIo4"
      },
      "outputs": [],
      "source": [
        "# Please adjust the training epochs count, and the other hyperparams (lr, dropout, ...), for a non-overfitted training according to your own needs.\n",
        "# tip: use tensorboard to display the accuracy (see cells above for tensorboard usage)\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "train(model=model, criterion=criterion, optimizer=optimizer, dataloader=train_dataloader,\n",
        "      x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test,\n",
        "      num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiC4nQ23wIo7"
      },
      "source": [
        "### 6. (When you're happy with the training:) Save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AJfKUyZwIo8"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/gesture_trained_100_30.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlxFW_9hwIpA"
      },
      "source": [
        "### 7. Get a trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sjDDnDlwIpB"
      },
      "outputs": [],
      "source": [
        "# Reminder: first redefine/load the HandGestureNet class before you use it, if you want to use it elsewhere\n",
        "model = HandGestureNet(n_channels=n_channels, n_classes=14)\n",
        "model.load_state_dict(torch.load('gesture_pretrained_model.pt'))\n",
        "# If error in load check if current runtime have GPU... maybe unavalable at runtime, use the line bellow instead.\n",
        "# model.load_state_dict(torch.load('gesture_pretrained_model.pt',torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# make predictions\n",
        "with torch.no_grad():\n",
        "    demo_gesture_batch = torch.randn(32, duration, n_channels)\n",
        "    predictions = model(demo_gesture_batch)\n",
        "    _, predictions = predictions.max(dim=1)\n",
        "    print(\"Predicted gesture classes: {}\".format(predictions.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcKckxpkwIpD"
      },
      "outputs": [],
      "source": [
        "# play with the model!\n",
        "n_classes = 15\n",
        "duration = 100\n",
        "n_channels = 66\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDxTk-B91VOE"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "# num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=9 * n_channels * 12, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=1936, out_features=n_classes)\n",
        "        )\n",
        "\n",
        "# Xavier initialization (copyed from main paper code)\n",
        "for layer in model.fc:\n",
        "            if layer.__class__.__name__ == \"Linear\":\n",
        "                torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "                torch.nn.init.constant_(layer.bias, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# Loss function & Optimizer\n",
        "# -----------------------------------------------------\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Please adjust the training epochs count, and the other hyperparams (lr, dropout, ...), for a non-overfitted training according to your own needs.\n",
        "# tip: use tensorboard to display the accuracy (see cells above for tensorboard usage)\n",
        "\n",
        "train(model=model, criterion=criterion, optimizer=optimizer, dataloader=train_dataloader,\n",
        "      x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "GpmaFCoasVC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix Plots"
      ],
      "metadata": {
        "id": "93j3B7ndwe6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "# make predictions\n",
        "with torch.no_grad():\n",
        "    yhat_test = model(x_test)\n",
        "    _, yhat_test = yhat_test.max(dim=1)\n",
        "    print(\"Predicted gesture classes: {}\".format(yhat_test.tolist()))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "cf_matrix = confusion_matrix(y_test, yhat_test)\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "metadata": {
        "id": "IuB7-yibs1On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data= pd.read_csv(\"prediction_balanced_medium_dataset.csv\")\n",
        "data"
      ],
      "metadata": {
        "id": "goVCY36BPWkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Test Accuracy Plots"
      ],
      "metadata": {
        "id": "Yz0zad2P9YD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"prediction_balanced_medium_dataset.csv\")\n",
        "\n",
        "properties = list(df.columns.values)\n",
        "properties.remove('Unnamed: 0')\n",
        "\n",
        "accuracy_train = df['Accuracy Train']\n",
        "accuracy_test = df['Accuracy Test']\n"
      ],
      "metadata": {
        "id": "85t0yjz-fFUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(accuracy_train)\n",
        "# print(accuracy_test)\n",
        "epochs = range(1,26)\n",
        "plt.plot(epochs, accuracy_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, accuracy_test, 'b', label='Test accuracy')\n",
        "plt.title('Training and Test accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V-jwmglde-qb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PMBA Model_trainning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}